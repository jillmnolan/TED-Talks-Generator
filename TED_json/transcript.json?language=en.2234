{"paragraphs":[{"cues":[{"time":739,"text":"So, I started my first job\nas a computer programmer"},{"time":4885,"text":"in my very first year of college --"},{"time":6865,"text":"basically, as a teenager."}]},{"cues":[{"time":8889,"text":"Soon after I started working,"},{"time":10645,"text":"writing software in a company,"},{"time":12799,"text":"a manager who worked at the company\ncame down to where I was,"},{"time":16458,"text":"and he whispered to me,"},{"time":18229,"text":"\"Can he tell if I'm lying?\""},{"time":21806,"text":"There was nobody else in the room."}]},{"cues":[{"time":25032,"text":"\"Can who tell if you're lying?\nAnd why are we whispering?\""}]},{"cues":[{"time":30266,"text":"The manager pointed\nat the computer in the room."},{"time":33397,"text":"\"Can he tell if I'm lying?\""},{"time":37613,"text":"Well, that manager was having\nan affair with the receptionist."}]},{"cues":[{"time":41999,"text":"(Laughter)"}]},{"cues":[{"time":43135,"text":"And I was still a teenager."},{"time":45447,"text":"So I whisper-shouted back to him,"},{"time":47490,"text":"\"Yes, the computer can tell\nif you're lying.\""}]},{"cues":[{"time":51138,"text":"(Laughter)"}]},{"cues":[{"time":52968,"text":"Well, I laughed, but actually,\nthe laugh's on me."},{"time":55915,"text":"Nowadays, there are computational systems"},{"time":59207,"text":"that can suss out\nemotional states and even lying"},{"time":62779,"text":"from processing human faces."},{"time":65248,"text":"Advertisers and even governments\nare very interested."}]},{"cues":[{"time":70319,"text":"I had become a computer programmer"},{"time":72205,"text":"because I was one of those kids\ncrazy about math and science."},{"time":75942,"text":"But somewhere along the line\nI'd learned about nuclear weapons,"},{"time":79074,"text":"and I'd gotten really concerned\nwith the ethics of science."},{"time":82050,"text":"I was troubled."},{"time":83278,"text":"However, because of family circumstances,"},{"time":85943,"text":"I also needed to start working\nas soon as possible."},{"time":89265,"text":"So I thought to myself, hey,\nlet me pick a technical field"},{"time":92588,"text":"where I can get a job easily"},{"time":94408,"text":"and where I don't have to deal\nwith any troublesome questions of ethics."},{"time":99022,"text":"So I picked computers."}]},{"cues":[{"time":100575,"text":"(Laughter)"}]},{"cues":[{"time":101703,"text":"Well, ha, ha, ha!\nAll the laughs are on me."},{"time":105137,"text":"Nowadays, computer scientists\nare building platforms"},{"time":107915,"text":"that control what a billion\npeople see every day."},{"time":113052,"text":"They're developing cars\nthat could decide who to run over."},{"time":117707,"text":"They're even building machines, weapons,"},{"time":120944,"text":"that might kill human beings in war."},{"time":123253,"text":"It's ethics all the way down."}]},{"cues":[{"time":127183,"text":"Machine intelligence is here."},{"time":129823,"text":"We're now using computation\nto make all sort of decisions,"},{"time":133321,"text":"but also new kinds of decisions."},{"time":135231,"text":"We're asking questions to computation\nthat have no single right answers,"},{"time":140427,"text":"that are subjective"},{"time":141653,"text":"and open-ended and value-laden."}]},{"cues":[{"time":144002,"text":"We're asking questions like,"},{"time":145784,"text":"\"Who should the company hire?\""},{"time":148096,"text":"\"Which update from which friend\nshould you be shown?\""},{"time":150879,"text":"\"Which convict is more\nlikely to reoffend?\""},{"time":153514,"text":"\"Which news item or movie\nshould be recommended to people?\""}]},{"cues":[{"time":156592,"text":"Look, yes, we've been using\ncomputers for a while,"},{"time":159988,"text":"but this is different."},{"time":161529,"text":"This is a historical twist,"},{"time":163620,"text":"because we cannot anchor computation\nfor such subjective decisions"},{"time":168981,"text":"the way we can anchor computation\nfor flying airplanes, building bridges,"},{"time":174425,"text":"going to the moon."},{"time":176449,"text":"Are airplanes safer?\nDid the bridge sway and fall?"},{"time":179732,"text":"There, we have agreed-upon,\nfairly clear benchmarks,"},{"time":184254,"text":"and we have laws of nature to guide us."},{"time":186517,"text":"We have no such anchors and benchmarks"},{"time":189935,"text":"for decisions in messy human affairs."}]},{"cues":[{"time":193922,"text":"To make things more complicated,\nour software is getting more powerful,"},{"time":198183,"text":"but it's also getting less\ntransparent and more complex."},{"time":202542,"text":"Recently, in the past decade,"},{"time":204606,"text":"complex algorithms\nhave made great strides."},{"time":207359,"text":"They can recognize human faces."},{"time":209985,"text":"They can decipher handwriting."},{"time":212436,"text":"They can detect credit card fraud"},{"time":214526,"text":"and block spam"},{"time":215739,"text":"and they can translate between languages."},{"time":217800,"text":"They can detect tumors in medical imaging."},{"time":220398,"text":"They can beat humans in chess and Go."}]},{"cues":[{"time":223264,"text":"Much of this progress comes\nfrom a method called \"machine learning.\""},{"time":228175,"text":"Machine learning is different\nthan traditional programming,"},{"time":231386,"text":"where you give the computer\ndetailed, exact, painstaking instructions."},{"time":235378,"text":"It's more like you take the system\nand you feed it lots of data,"},{"time":239584,"text":"including unstructured data,"},{"time":241264,"text":"like the kind we generate\nin our digital lives."},{"time":243566,"text":"And the system learns\nby churning through this data."},{"time":246669,"text":"And also, crucially,"},{"time":248219,"text":"these systems don't operate\nunder a single-answer logic."},{"time":252623,"text":"They don't produce a simple answer;\nit's more probabilistic:"},{"time":255606,"text":"\"This one is probably more like\nwhat you're looking for.\""}]},{"cues":[{"time":260023,"text":"Now, the upside is:\nthis method is really powerful."},{"time":263117,"text":"The head of Google's AI systems called it,"},{"time":265217,"text":"\"the unreasonable effectiveness of data.\""},{"time":267791,"text":"The downside is,"},{"time":269738,"text":"we don't really understand\nwhat the system learned."},{"time":272833,"text":"In fact, that's its power."},{"time":274946,"text":"This is less like giving\ninstructions to a computer;"},{"time":279200,"text":"it's more like training\na puppy-machine-creature"},{"time":283288,"text":"we don't really understand or control."},{"time":286362,"text":"So this is our problem."},{"time":288427,"text":"It's a problem when this artificial\nintelligence system gets things wrong."},{"time":292713,"text":"It's also a problem\nwhen it gets things right,"},{"time":296277,"text":"because we don't even know which is which\nwhen it's a subjective problem."},{"time":299929,"text":"We don't know what this thing is thinking."}]},{"cues":[{"time":303493,"text":"So, consider a hiring algorithm --"},{"time":308123,"text":"a system used to hire people,\nusing machine-learning systems."},{"time":313052,"text":"Such a system would have been trained\non previous employees' data"},{"time":316655,"text":"and instructed to find and hire"},{"time":319270,"text":"people like the existing\nhigh performers in the company."},{"time":322814,"text":"Sounds good."},{"time":323991,"text":"I once attended a conference"},{"time":326014,"text":"that brought together\nhuman resources managers and executives,"},{"time":329163,"text":"high-level people,"},{"time":330393,"text":"using such systems in hiring."},{"time":331976,"text":"They were super excited."},{"time":333646,"text":"They thought that this would make hiring\nmore objective, less biased,"},{"time":338323,"text":"and give women\nand minorities a better shot"},{"time":341347,"text":"against biased human managers."}]},{"cues":[{"time":343559,"text":"And look -- human hiring is biased."},{"time":347099,"text":"I know."},{"time":348308,"text":"I mean, in one of my early jobs\nas a programmer,"},{"time":351337,"text":"my immediate manager would sometimes\ncome down to where I was"},{"time":355229,"text":"really early in the morning\nor really late in the afternoon,"},{"time":359006,"text":"and she'd say, \"Zeynep,\nlet's go to lunch!\""},{"time":362724,"text":"I'd be puzzled by the weird timing."},{"time":364915,"text":"It's 4pm. Lunch?"},{"time":367068,"text":"I was broke, so free lunch. I always went."},{"time":370618,"text":"I later realized what was happening."},{"time":372709,"text":"My immediate managers\nhad not confessed to their higher-ups"},{"time":377279,"text":"that the programmer they hired\nfor a serious job was a teen girl"},{"time":380416,"text":"who wore jeans and sneakers to work."},{"time":385174,"text":"I was doing a good job,\nI just looked wrong"},{"time":387400,"text":"and was the wrong age and gender."}]},{"cues":[{"time":389123,"text":"So hiring in a gender- and race-blind way"},{"time":392493,"text":"certainly sounds good to me."},{"time":395031,"text":"But with these systems,\nit is more complicated, and here's why:"},{"time":398968,"text":"Currently, computational systems\ncan infer all sorts of things about you"},{"time":404783,"text":"from your digital crumbs,"},{"time":406679,"text":"even if you have not\ndisclosed those things."},{"time":409506,"text":"They can infer your sexual orientation,"},{"time":412994,"text":"your personality traits,"},{"time":414859,"text":"your political leanings."},{"time":416830,"text":"They have predictive power\nwith high levels of accuracy."},{"time":421362,"text":"Remember -- for things\nyou haven't even disclosed."},{"time":423964,"text":"This is inference."}]},{"cues":[{"time":425579,"text":"I have a friend who developed\nsuch computational systems"},{"time":428864,"text":"to predict the likelihood\nof clinical or postpartum depression"},{"time":432529,"text":"from social media data."},{"time":434676,"text":"The results are impressive."},{"time":436492,"text":"Her system can predict\nthe likelihood of depression"},{"time":439873,"text":"months before the onset of any symptoms --"},{"time":443800,"text":"months before."},{"time":445197,"text":"No symptoms, there's prediction."},{"time":447467,"text":"She hopes it will be used\nfor early intervention. Great!"},{"time":452911,"text":"But now put this in the context of hiring."}]},{"cues":[{"time":456027,"text":"So at this human resources\nmanagers conference,"},{"time":459097,"text":"I approached a high-level manager\nin a very large company,"},{"time":463830,"text":"and I said to her, \"Look,\nwhat if, unbeknownst to you,"},{"time":468432,"text":"your system is weeding out people\nwith high future likelihood of depression?"},{"time":475761,"text":"They're not depressed now,\njust maybe in the future, more likely."},{"time":479923,"text":"What if it's weeding out women\nmore likely to be pregnant"},{"time":483353,"text":"in the next year or two\nbut aren't pregnant now?"},{"time":486844,"text":"What if it's hiring aggressive people\nbecause that's your workplace culture?\""},{"time":493173,"text":"You can't tell this by looking\nat gender breakdowns."},{"time":495888,"text":"Those may be balanced."},{"time":497414,"text":"And since this is machine learning,\nnot traditional coding,"},{"time":500995,"text":"there is no variable there\nlabeled \"higher risk of depression,\""},{"time":505926,"text":"\"higher risk of pregnancy,\""},{"time":507783,"text":"\"aggressive guy scale.\""},{"time":509995,"text":"Not only do you not know\nwhat your system is selecting on,"},{"time":513698,"text":"you don't even know\nwhere to begin to look."},{"time":516045,"text":"It's a black box."},{"time":517315,"text":"It has predictive power,\nbut you don't understand it."}]},{"cues":[{"time":520486,"text":"\"What safeguards,\" I asked, \"do you have"},{"time":522879,"text":"to make sure that your black box\nisn't doing something shady?\""},{"time":528863,"text":"She looked at me as if I had\njust stepped on 10 puppy tails."}]},{"cues":[{"time":532765,"text":"(Laughter)"}]},{"cues":[{"time":534037,"text":"She stared at me and she said,"},{"time":536556,"text":"\"I don't want to hear\nanother word about this.\""},{"time":541458,"text":"And she turned around and walked away."},{"time":544064,"text":"Mind you -- she wasn't rude."},{"time":545574,"text":"It was clearly: what I don't know\nisn't my problem, go away, death stare."}]},{"cues":[{"time":551906,"text":"(Laughter)"}]},{"cues":[{"time":553862,"text":"Look, such a system\nmay even be less biased"},{"time":557725,"text":"than human managers in some ways."},{"time":559852,"text":"And it could make monetary sense."},{"time":562573,"text":"But it could also lead"},{"time":564247,"text":"to a steady but stealthy\nshutting out of the job market"},{"time":569019,"text":"of people with higher risk of depression."},{"time":571753,"text":"Is this the kind of society\nwe want to build,"},{"time":574373,"text":"without even knowing we've done this,"},{"time":576682,"text":"because we turned decision-making\nto machines we don't totally understand?"}]},{"cues":[{"time":581265,"text":"Another problem is this:"},{"time":583314,"text":"these systems are often trained\non data generated by our actions,"},{"time":587790,"text":"human imprints."},{"time":590188,"text":"Well, they could just be\nreflecting our biases,"},{"time":594020,"text":"and these systems\ncould be picking up on our biases"},{"time":597637,"text":"and amplifying them"},{"time":598974,"text":"and showing them back to us,"},{"time":600416,"text":"while we're telling ourselves,"},{"time":601902,"text":"\"We're just doing objective,\nneutral computation.\""}]},{"cues":[{"time":606314,"text":"Researchers found that on Google,"},{"time":610134,"text":"women are less likely than men\nto be shown job ads for high-paying jobs."},{"time":616463,"text":"And searching for African-American names"},{"time":619017,"text":"is more likely to bring up ads\nsuggesting criminal history,"},{"time":623747,"text":"even when there is none."},{"time":626693,"text":"Such hidden biases\nand black-box algorithms"},{"time":630266,"text":"that researchers uncover sometimes\nbut sometimes we don't know,"},{"time":634263,"text":"can have life-altering consequences."}]},{"cues":[{"time":637958,"text":"In Wisconsin, a defendant\nwas sentenced to six years in prison"},{"time":642141,"text":"for evading the police."},{"time":644824,"text":"You may not know this,"},{"time":646034,"text":"but algorithms are increasingly used\nin parole and sentencing decisions."},{"time":650056,"text":"He wanted to know:\nHow is this score calculated?"},{"time":653795,"text":"It's a commercial black box."},{"time":655484,"text":"The company refused to have its algorithm\nbe challenged in open court."},{"time":660396,"text":"But ProPublica, an investigative\nnonprofit, audited that very algorithm"},{"time":665952,"text":"with what public data they could find,"},{"time":667992,"text":"and found that its outcomes were biased"},{"time":670332,"text":"and its predictive power\nwas dismal, barely better than chance,"},{"time":673985,"text":"and it was wrongly labeling\nblack defendants as future criminals"},{"time":678425,"text":"at twice the rate of white defendants."}]},{"cues":[{"time":683891,"text":"So, consider this case:"},{"time":686103,"text":"This woman was late\npicking up her godsister"},{"time":689979,"text":"from a school in Broward County, Florida,"},{"time":692757,"text":"running down the street\nwith a friend of hers."},{"time":695137,"text":"They spotted an unlocked kid's bike\nand a scooter on a porch"},{"time":699260,"text":"and foolishly jumped on it."},{"time":700916,"text":"As they were speeding off,\na woman came out and said,"},{"time":703539,"text":"\"Hey! That's my kid's bike!\""},{"time":705768,"text":"They dropped it, they walked away,\nbut they were arrested."}]},{"cues":[{"time":709086,"text":"She was wrong, she was foolish,\nbut she was also just 18."},{"time":712747,"text":"She had a couple of juvenile misdemeanors."},{"time":715808,"text":"Meanwhile, that man had been arrested\nfor shoplifting in Home Depot --"},{"time":721017,"text":"85 dollars' worth of stuff,\na similar petty crime."},{"time":724766,"text":"But he had two prior\narmed robbery convictions."},{"time":729955,"text":"But the algorithm scored her\nas high risk, and not him."},{"time":734746,"text":"Two years later, ProPublica found\nthat she had not reoffended."},{"time":738644,"text":"It was just hard to get a job\nfor her with her record."},{"time":741218,"text":"He, on the other hand, did reoffend"},{"time":743318,"text":"and is now serving an eight-year\nprison term for a later crime."},{"time":748088,"text":"Clearly, we need to audit our black boxes"},{"time":751481,"text":"and not have them have\nthis kind of unchecked power."}]},{"cues":[{"time":754120,"text":"(Applause)"}]},{"cues":[{"time":758087,"text":"Audits are great and important,\nbut they don't solve all our problems."},{"time":762353,"text":"Take Facebook's powerful\nnews feed algorithm --"},{"time":765125,"text":"you know, the one that ranks everything\nand decides what to show you"},{"time":769992,"text":"from all the friends and pages you follow."},{"time":772898,"text":"Should you be shown another baby picture?"}]},{"cues":[{"time":775197,"text":"(Laughter)"}]},{"cues":[{"time":776417,"text":"A sullen note from an acquaintance?"},{"time":779449,"text":"An important but difficult news item?"},{"time":781329,"text":"There's no right answer."},{"time":782835,"text":"Facebook optimizes\nfor engagement on the site:"},{"time":785518,"text":"likes, shares, comments."}]},{"cues":[{"time":788168,"text":"In August of 2014,"},{"time":790888,"text":"protests broke out in Ferguson, Missouri,"},{"time":793574,"text":"after the killing of an African-American\nteenager by a white police officer,"},{"time":798015,"text":"under murky circumstances."},{"time":799974,"text":"The news of the protests was all over"},{"time":802005,"text":"my algorithmically\nunfiltered Twitter feed,"},{"time":804714,"text":"but nowhere on my Facebook."},{"time":807182,"text":"Was it my Facebook friends?"},{"time":808940,"text":"I disabled Facebook's algorithm,"},{"time":811472,"text":"which is hard because Facebook\nkeeps wanting to make you"},{"time":814344,"text":"come under the algorithm's control,"},{"time":816404,"text":"and saw that my friends\nwere talking about it."},{"time":818666,"text":"It's just that the algorithm\nwasn't showing it to me."},{"time":821199,"text":"I researched this and found\nthis was a widespread problem."}]},{"cues":[{"time":824265,"text":"The story of Ferguson\nwasn't algorithm-friendly."},{"time":828102,"text":"It's not \"likable.\""},{"time":829297,"text":"Who's going to click on \"like?\""},{"time":831500,"text":"It's not even easy to comment on."},{"time":833730,"text":"Without likes and comments,"},{"time":835125,"text":"the algorithm was likely showing it\nto even fewer people,"},{"time":838441,"text":"so we didn't get to see this."},{"time":840946,"text":"Instead, that week,"},{"time":842198,"text":"Facebook's algorithm highlighted this,"},{"time":844520,"text":"which is the ALS Ice Bucket Challenge."},{"time":846770,"text":"Worthy cause; dump ice water,\ndonate to charity, fine."},{"time":850536,"text":"But it was super algorithm-friendly."},{"time":853219,"text":"The machine made this decision for us."},{"time":855856,"text":"A very important\nbut difficult conversation"},{"time":859377,"text":"might have been smothered,"},{"time":860956,"text":"had Facebook been the only channel."}]},{"cues":[{"time":864117,"text":"Now, finally, these systems\ncan also be wrong"},{"time":867938,"text":"in ways that don't resemble human systems."},{"time":870698,"text":"Do you guys remember Watson,\nIBM's machine-intelligence system"},{"time":873644,"text":"that wiped the floor\nwith human contestants on Jeopardy?"},{"time":877131,"text":"It was a great player."},{"time":878583,"text":"But then, for Final Jeopardy,\nWatson was asked this question:"},{"time":882659,"text":"\"Its largest airport is named\nfor a World War II hero,"},{"time":885615,"text":"its second-largest\nfor a World War II battle.\""}]},{"cues":[{"time":887891,"text":"(Hums Final Jeopardy music)"}]},{"cues":[{"time":889582,"text":"Chicago."},{"time":890788,"text":"The two humans got it right."},{"time":892697,"text":"Watson, on the other hand,\nanswered \"Toronto\" --"},{"time":897069,"text":"for a US city category!"},{"time":899596,"text":"The impressive system also made an error"},{"time":902521,"text":"that a human would never make,\na second-grader wouldn't make."}]},{"cues":[{"time":906823,"text":"Our machine intelligence can fail"},{"time":909956,"text":"in ways that don't fit\nerror patterns of humans,"},{"time":913080,"text":"in ways we won't expect\nand be prepared for."},{"time":916054,"text":"It'd be lousy not to get a job\none is qualified for,"},{"time":919716,"text":"but it would triple suck\nif it was because of stack overflow"},{"time":923467,"text":"in some subroutine."}]},{"cues":[{"time":924923,"text":"(Laughter)"}]},{"cues":[{"time":926526,"text":"In May of 2010,"},{"time":929336,"text":"a flash crash on Wall Street\nfueled by a feedback loop"},{"time":933404,"text":"in Wall Street's \"sell\" algorithm"},{"time":936456,"text":"wiped a trillion dollars\nof value in 36 minutes."},{"time":941722,"text":"I don't even want to think\nwhat \"error\" means"},{"time":943933,"text":"in the context of lethal\nautonomous weapons."}]},{"cues":[{"time":949894,"text":"So yes, humans have always made biases."},{"time":953708,"text":"Decision makers and gatekeepers,"},{"time":955908,"text":"in courts, in news, in war ..."},{"time":959425,"text":"they make mistakes;\nbut that's exactly my point."},{"time":962487,"text":"We cannot escape\nthese difficult questions."},{"time":966596,"text":"We cannot outsource\nour responsibilities to machines."}]},{"cues":[{"time":970676,"text":"(Applause)"}]},{"cues":[{"time":977089,"text":"Artificial intelligence does not give us\na \"Get out of ethics free\" card."}]},{"cues":[{"time":982742,"text":"Data scientist Fred Benenson\ncalls this math-washing."},{"time":986147,"text":"We need the opposite."},{"time":987560,"text":"We need to cultivate algorithm suspicion,\nscrutiny and investigation."},{"time":993380,"text":"We need to make sure we have\nalgorithmic accountability,"},{"time":996602,"text":"auditing and meaningful transparency."},{"time":999380,"text":"We need to accept\nthat bringing math and computation"},{"time":1002638,"text":"to messy, value-laden human affairs"},{"time":1005632,"text":"does not bring objectivity;"},{"time":1008040,"text":"rather, the complexity of human affairs\ninvades the algorithms."},{"time":1012148,"text":"Yes, we can and we should use computation"},{"time":1015659,"text":"to help us make better decisions."},{"time":1017697,"text":"But we have to own up\nto our moral responsibility to judgment,"},{"time":1023053,"text":"and use algorithms within that framework,"},{"time":1025895,"text":"not as a means to abdicate\nand outsource our responsibilities"},{"time":1030854,"text":"to one another as human to human."}]},{"cues":[{"time":1033807,"text":"Machine intelligence is here."},{"time":1036440,"text":"That means we must hold on ever tighter"},{"time":1039885,"text":"to human values and human ethics."}]},{"cues":[{"time":1042056,"text":"Thank you."}]},{"cues":[{"time":1043234,"text":"(Applause)"}]}]}