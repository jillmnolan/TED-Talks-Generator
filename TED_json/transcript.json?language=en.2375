{"paragraphs":[{"cues":[{"time":712,"text":"This is Lee Sedol."},{"time":2288,"text":"Lee Sedol is one of the world's\ngreatest Go players,"},{"time":6309,"text":"and he's having what my friends\nin Silicon Valley call"},{"time":9218,"text":"a \"Holy Cow\" moment --"}]},{"cues":[{"time":10752,"text":"(Laughter)"}]},{"cues":[{"time":11849,"text":"a moment where we realize"},{"time":14061,"text":"that AI is actually progressing\na lot faster than we expected."},{"time":18154,"text":"So humans have lost on the Go board.\nWhat about the real world?"}]},{"cues":[{"time":21225,"text":"Well, the real world is much bigger,"},{"time":23349,"text":"much more complicated than the Go board."},{"time":25622,"text":"It's a lot less visible,"},{"time":27465,"text":"but it's still a decision problem."},{"time":30948,"text":"And if we think about some\nof the technologies"},{"time":33293,"text":"that are coming down the pike ..."},{"time":35738,"text":"Noriko [Arai] mentioned that reading\nis not yet happening in machines,"},{"time":40097,"text":"at least with understanding."},{"time":41621,"text":"But that will happen,"},{"time":43181,"text":"and when that happens,"},{"time":44976,"text":"very soon afterwards,"},{"time":46187,"text":"machines will have read everything\nthat the human race has ever written."},{"time":51850,"text":"And that will enable machines,"},{"time":53904,"text":"along with the ability to look\nfurther ahead than humans can,"},{"time":56848,"text":"as we've already seen in Go,"},{"time":58552,"text":"if they also have access\nto more information,"},{"time":60740,"text":"they'll be able to make better decisions\nin the real world than we can."},{"time":66792,"text":"So is that a good thing?"},{"time":69898,"text":"Well, I hope so."}]},{"cues":[{"time":74694,"text":"Our entire civilization,\neverything that we value,"},{"time":77973,"text":"is based on our intelligence."},{"time":80065,"text":"And if we had access\nto a lot more intelligence,"},{"time":83783,"text":"then there's really no limit\nto what the human race can do."},{"time":88665,"text":"And I think this could be,\nas some people have described it,"},{"time":92014,"text":"the biggest event in human history."},{"time":96665,"text":"So why are people saying things like this,"},{"time":99518,"text":"that AI might spell the end\nof the human race?"},{"time":103438,"text":"Is this a new thing?"},{"time":105121,"text":"Is it just Elon Musk and Bill Gates\nand Stephen Hawking?"}]},{"cues":[{"time":109953,"text":"Actually, no. This idea\nhas been around for a while."},{"time":113239,"text":"Here's a quotation:"},{"time":115225,"text":"\"Even if we could keep the machines\nin a subservient position,"},{"time":119599,"text":"for instance, by turning off the power\nat strategic moments\" --"},{"time":122607,"text":"and I'll come back to that\n\"turning off the power\" idea later on --"},{"time":125868,"text":"\"we should, as a species,\nfeel greatly humbled.\""},{"time":130177,"text":"So who said this?\nThis is Alan Turing in 1951."},{"time":134300,"text":"Alan Turing, as you know,\nis the father of computer science"},{"time":137087,"text":"and in many ways,\nthe father of AI as well."},{"time":141239,"text":"So if we think about this problem,"},{"time":143145,"text":"the problem of creating something\nmore intelligent than your own species,"},{"time":146956,"text":"we might call this \"the gorilla problem,\""},{"time":150345,"text":"because gorillas' ancestors did this\na few million years ago,"},{"time":154119,"text":"and now we can ask the gorillas:"},{"time":156752,"text":"Was this a good idea?"}]},{"cues":[{"time":157936,"text":"So here they are having a meeting\nto discuss whether it was a good idea,"},{"time":161490,"text":"and after a little while,\nthey conclude, no,"},{"time":164860,"text":"this was a terrible idea."},{"time":166229,"text":"Our species is in dire straits."},{"time":168538,"text":"In fact, you can see the existential\nsadness in their eyes."}]},{"cues":[{"time":172825,"text":"(Laughter)"}]},{"cues":[{"time":174489,"text":"So this queasy feeling that making\nsomething smarter than your own species"},{"time":179353,"text":"is maybe not a good idea --"},{"time":182488,"text":"what can we do about that?"},{"time":184003,"text":"Well, really nothing,\nexcept stop doing AI,"},{"time":188794,"text":"and because of all\nthe benefits that I mentioned"},{"time":191328,"text":"and because I'm an AI researcher,"},{"time":193068,"text":"I'm not having that."},{"time":195283,"text":"I actually want to be able\nto keep doing AI."}]},{"cues":[{"time":198615,"text":"So we actually need to nail down\nthe problem a bit more."},{"time":201317,"text":"What exactly is the problem?"},{"time":202712,"text":"Why is better AI possibly a catastrophe?"}]},{"cues":[{"time":207398,"text":"So here's another quotation:"},{"time":209935,"text":"\"We had better be quite sure\nthat the purpose put into the machine"},{"time":213294,"text":"is the purpose which we really desire.\""},{"time":216282,"text":"This was said by Norbert Wiener in 1960,"},{"time":219804,"text":"shortly after he watched\none of the very early learning systems"},{"time":223830,"text":"learn to play checkers\nbetter than its creator."},{"time":228602,"text":"But this could equally have been said"},{"time":231309,"text":"by King Midas."},{"time":233083,"text":"King Midas said, \"I want everything\nI touch to turn to gold,\""},{"time":236241,"text":"and he got exactly what he asked for."},{"time":238738,"text":"That was the purpose\nthat he put into the machine,"},{"time":241513,"text":"so to speak,"},{"time":242987,"text":"and then his food and his drink\nand his relatives turned to gold"},{"time":246455,"text":"and he died in misery and starvation."},{"time":250444,"text":"So we'll call this\n\"the King Midas problem\""},{"time":252809,"text":"of stating an objective\nwhich is not, in fact,"},{"time":256138,"text":"truly aligned with what we want."},{"time":258575,"text":"In modern terms, we call this\n\"the value alignment problem.\""}]},{"cues":[{"time":265047,"text":"Putting in the wrong objective\nis not the only part of the problem."},{"time":268556,"text":"There's another part."},{"time":270160,"text":"If you put an objective into a machine,"},{"time":272127,"text":"even something as simple as,\n\"Fetch the coffee,\""},{"time":275908,"text":"the machine says to itself,"},{"time":278733,"text":"\"Well, how might I fail\nto fetch the coffee?"},{"time":281380,"text":"Someone might switch me off."},{"time":283645,"text":"OK, I have to take steps to prevent that."},{"time":286056,"text":"I will disable my 'off' switch."},{"time":288534,"text":"I will do anything to defend myself\nagainst interference"},{"time":291517,"text":"with this objective\nthat I have been given.\""},{"time":294170,"text":"So this single-minded pursuit"},{"time":297213,"text":"in a very defensive mode\nof an objective that is, in fact,"},{"time":300182,"text":"not aligned with the true objectives\nof the human race --"},{"time":304122,"text":"that's the problem that we face."},{"time":307007,"text":"And in fact, that's the high-value\ntakeaway from this talk."},{"time":311798,"text":"If you want to remember one thing,"},{"time":313877,"text":"it's that you can't fetch\nthe coffee if you're dead."}]},{"cues":[{"time":316576,"text":"(Laughter)"}]},{"cues":[{"time":317661,"text":"It's very simple. Just remember that.\nRepeat it to yourself three times a day."}]},{"cues":[{"time":321514,"text":"(Laughter)"}]},{"cues":[{"time":323359,"text":"And in fact, this is exactly the plot"},{"time":326137,"text":"of \"2001: [A Space Odyssey]\""},{"time":329226,"text":"HAL has an objective, a mission,"},{"time":331340,"text":"which is not aligned\nwith the objectives of the humans,"},{"time":335096,"text":"and that leads to this conflict."},{"time":337494,"text":"Now fortunately, HAL\nis not superintelligent."},{"time":340487,"text":"He's pretty smart,\nbut eventually Dave outwits him"},{"time":344098,"text":"and manages to switch him off."},{"time":349828,"text":"But we might not be so lucky."},{"time":356193,"text":"So what are we going to do?"}]},{"cues":[{"time":360371,"text":"I'm trying to redefine AI"},{"time":362996,"text":"to get away from this classical notion"},{"time":365081,"text":"of machines that intelligently\npursue objectives."},{"time":370712,"text":"There are three principles involved."},{"time":372534,"text":"The first one is a principle\nof altruism, if you like,"},{"time":375847,"text":"that the robot's only objective"},{"time":379133,"text":"is to maximize the realization\nof human objectives,"},{"time":383403,"text":"of human values."},{"time":384817,"text":"And by values here I don't mean\ntouchy-feely, goody-goody values."},{"time":388171,"text":"I just mean whatever it is\nthat the human would prefer"},{"time":391982,"text":"their life to be like."},{"time":395364,"text":"And so this actually violates Asimov's law"},{"time":397697,"text":"that the robot has to protect\nits own existence."},{"time":400050,"text":"It has no interest in preserving\nits existence whatsoever."}]},{"cues":[{"time":405420,"text":"The second law is a law\nof humility, if you like."},{"time":409974,"text":"And this turns out to be really\nimportant to make robots safe."},{"time":413741,"text":"It says that the robot does not know"},{"time":416907,"text":"what those human values are,"},{"time":418959,"text":"so it has to maximize them,\nbut it doesn't know what they are."},{"time":423254,"text":"And that avoids this problem\nof single-minded pursuit"},{"time":425904,"text":"of an objective."},{"time":427140,"text":"This uncertainty turns out to be crucial."}]},{"cues":[{"time":429726,"text":"Now, in order to be useful to us,"},{"time":431389,"text":"it has to have some idea of what we want."},{"time":435223,"text":"It obtains that information primarily\nby observation of human choices,"},{"time":440674,"text":"so our own choices reveal information"},{"time":443499,"text":"about what it is that we prefer\nour lives to be like."},{"time":448632,"text":"So those are the three principles."},{"time":450339,"text":"Let's see how that applies\nto this question of:"},{"time":452681,"text":"\"Can you switch the machine off?\"\nas Turing suggested."}]},{"cues":[{"time":457073,"text":"So here's a PR2 robot."},{"time":459217,"text":"This is one that we have in our lab,"},{"time":461062,"text":"and it has a big red \"off\" switch\nright on the back."},{"time":464541,"text":"The question is: Is it\ngoing to let you switch it off?"},{"time":467180,"text":"If we do it the classical way,"},{"time":468669,"text":"we give it the objective of, \"Fetch\nthe coffee, I must fetch the coffee,"},{"time":472175,"text":"I can't fetch the coffee if I'm dead,\""},{"time":474779,"text":"so obviously the PR2\nhas been listening to my talk,"},{"time":478144,"text":"and so it says, therefore,\n\"I must disable my 'off' switch,"},{"time":482976,"text":"and probably taser all the other\npeople in Starbucks"},{"time":485694,"text":"who might interfere with me.\""}]},{"cues":[{"time":487278,"text":"(Laughter)"}]},{"cues":[{"time":489364,"text":"So this seems to be inevitable, right?"},{"time":491541,"text":"This kind of failure mode\nseems to be inevitable,"},{"time":493963,"text":"and it follows from having\na concrete, definite objective."}]},{"cues":[{"time":498812,"text":"So what happens if the machine\nis uncertain about the objective?"},{"time":501980,"text":"Well, it reasons in a different way."},{"time":504131,"text":"It says, \"OK, the human\nmight switch me off,"},{"time":507144,"text":"but only if I'm doing something wrong."},{"time":509747,"text":"Well, I don't really know what wrong is,"},{"time":512246,"text":"but I know that I don't want to do it.\""},{"time":514314,"text":"So that's the first and second\nprinciples right there."},{"time":517348,"text":"\"So I should let the human switch me off.\""},{"time":521721,"text":"And in fact you can calculate\nthe incentive that the robot has"},{"time":525701,"text":"to allow the human to switch it off,"},{"time":528218,"text":"and it's directly tied to the degree"},{"time":530156,"text":"of uncertainty about\nthe underlying objective."}]},{"cues":[{"time":533977,"text":"And then when the machine is switched off,"},{"time":536950,"text":"that third principle comes into play."},{"time":538779,"text":"It learns something about the objectives\nit should be pursuing,"},{"time":541865,"text":"because it learns that\nwhat it did wasn't right."},{"time":544422,"text":"In fact, we can, with suitable use\nof Greek symbols,"},{"time":548016,"text":"as mathematicians usually do,"},{"time":550171,"text":"we can actually prove a theorem"},{"time":552179,"text":"that says that such a robot\nis provably beneficial to the human."},{"time":555756,"text":"You are provably better off\nwith a machine that's designed in this way"},{"time":559583,"text":"than without it."},{"time":561237,"text":"So this is a very simple example,\nbut this is the first step"},{"time":564167,"text":"in what we're trying to do\nwith human-compatible AI."}]},{"cues":[{"time":570657,"text":"Now, this third principle,"},{"time":573938,"text":"I think is the one that you're probably\nscratching your head over."},{"time":577074,"text":"You're probably thinking, \"Well,\nyou know, I behave badly."},{"time":580337,"text":"I don't want my robot to behave like me."},{"time":583290,"text":"I sneak down in the middle of the night\nand take stuff from the fridge."},{"time":586748,"text":"I do this and that.\""},{"time":587940,"text":"There's all kinds of things\nyou don't want the robot doing."},{"time":590761,"text":"But in fact, it doesn't\nquite work that way."},{"time":592856,"text":"Just because you behave badly"},{"time":595035,"text":"doesn't mean the robot\nis going to copy your behavior."},{"time":597682,"text":"It's going to understand your motivations\nand maybe help you resist them,"},{"time":601616,"text":"if appropriate."},{"time":604206,"text":"But it's still difficult."},{"time":606302,"text":"What we're trying to do, in fact,"},{"time":608871,"text":"is to allow machines to predict\nfor any person and for any possible life"},{"time":614691,"text":"that they could live,"},{"time":615876,"text":"and the lives of everybody else:"},{"time":617497,"text":"Which would they prefer?"},{"time":622061,"text":"And there are many, many\ndifficulties involved in doing this;"},{"time":625039,"text":"I don't expect that this\nis going to get solved very quickly."},{"time":627995,"text":"The real difficulties, in fact, are us."}]},{"cues":[{"time":632149,"text":"As I have already mentioned,\nwe behave badly."},{"time":635290,"text":"In fact, some of us are downright nasty."},{"time":638431,"text":"Now the robot, as I said,\ndoesn't have to copy the behavior."},{"time":641507,"text":"The robot does not have\nany objective of its own."},{"time":644322,"text":"It's purely altruistic."},{"time":647293,"text":"And it's not designed just to satisfy\nthe desires of one person, the user,"},{"time":652538,"text":"but in fact it has to respect\nthe preferences of everybody."},{"time":657263,"text":"So it can deal with a certain\namount of nastiness,"},{"time":659857,"text":"and it can even understand\nthat your nastiness, for example,"},{"time":663582,"text":"you may take bribes as a passport official"},{"time":666277,"text":"because you need to feed your family\nand send your kids to school."},{"time":670113,"text":"It can understand that;\nit doesn't mean it's going to steal."},{"time":673043,"text":"In fact, it'll just help you\nsend your kids to school."}]},{"cues":[{"time":676976,"text":"We are also computationally limited."},{"time":680012,"text":"Lee Sedol is a brilliant Go player,"},{"time":682541,"text":"but he still lost."},{"time":683890,"text":"So if we look at his actions,\nhe took an action that lost the game."},{"time":688153,"text":"That doesn't mean he wanted to lose."},{"time":691340,"text":"So to understand his behavior,"},{"time":693404,"text":"we actually have to invert\nthrough a model of human cognition"},{"time":697072,"text":"that includes our computational\nlimitations -- a very complicated model."},{"time":702073,"text":"But it's still something\nthat we can work on understanding."}]},{"cues":[{"time":705876,"text":"Probably the most difficult part,\nfrom my point of view as an AI researcher,"},{"time":710220,"text":"is the fact that there are lots of us,"},{"time":714294,"text":"and so the machine has to somehow\ntrade off, weigh up the preferences"},{"time":717899,"text":"of many different people,"},{"time":720148,"text":"and there are different ways to do that."},{"time":722078,"text":"Economists, sociologists,\nmoral philosophers have understood that,"},{"time":725791,"text":"and we are actively\nlooking for collaboration."}]},{"cues":[{"time":728270,"text":"Let's have a look and see what happens\nwhen you get that wrong."},{"time":731545,"text":"So you can have\na conversation, for example,"},{"time":733702,"text":"with your intelligent personal assistant"},{"time":735670,"text":"that might be available\nin a few years' time."},{"time":737979,"text":"Think of a Siri on steroids."},{"time":741627,"text":"So Siri says, \"Your wife called\nto remind you about dinner tonight.\""},{"time":746616,"text":"And of course, you've forgotten.\n\"What? What dinner?"},{"time":749148,"text":"What are you talking about?\""}]},{"cues":[{"time":750597,"text":"\"Uh, your 20th anniversary at 7pm.\""}]},{"cues":[{"time":756915,"text":"\"I can't do that. I'm meeting\nwith the secretary-general at 7:30."},{"time":760658,"text":"How could this have happened?\""}]},{"cues":[{"time":762374,"text":"\"Well, I did warn you, but you overrode\nmy recommendation.\""}]},{"cues":[{"time":768146,"text":"\"Well, what am I going to do?\nI can't just tell him I'm too busy.\""}]},{"cues":[{"time":772490,"text":"\"Don't worry. I arranged\nfor his plane to be delayed.\""}]},{"cues":[{"time":775795,"text":"(Laughter)"}]},{"cues":[{"time":778249,"text":"\"Some kind of computer malfunction.\""}]},{"cues":[{"time":780374,"text":"(Laughter)"}]},{"cues":[{"time":781610,"text":"\"Really? You can do that?\""}]},{"cues":[{"time":784400,"text":"\"He sends his profound apologies"},{"time":786603,"text":"and looks forward to meeting you\nfor lunch tomorrow.\""}]},{"cues":[{"time":789182,"text":"(Laughter)"}]},{"cues":[{"time":790505,"text":"So the values here --\nthere's a slight mistake going on."},{"time":794932,"text":"This is clearly following my wife's values"},{"time":797965,"text":"which is \"Happy wife, happy life.\""}]},{"cues":[{"time":800058,"text":"(Laughter)"}]},{"cues":[{"time":801665,"text":"It could go the other way."},{"time":803821,"text":"You could come home\nafter a hard day's work,"},{"time":806046,"text":"and the computer says, \"Long day?\""}]},{"cues":[{"time":808265,"text":"\"Yes, I didn't even have time for lunch.\""}]},{"cues":[{"time":810577,"text":"\"You must be very hungry.\""}]},{"cues":[{"time":811883,"text":"\"Starving, yeah.\nCould you make some dinner?\""}]},{"cues":[{"time":816070,"text":"\"There's something I need to tell you.\""}]},{"cues":[{"time":818184,"text":"(Laughter)"}]},{"cues":[{"time":820193,"text":"\"There are humans in South Sudan\nwho are in more urgent need than you.\""}]},{"cues":[{"time":825122,"text":"(Laughter)"}]},{"cues":[{"time":826250,"text":"\"So I'm leaving. Make your own dinner.\""}]},{"cues":[{"time":828349,"text":"(Laughter)"}]},{"cues":[{"time":830823,"text":"So we have to solve these problems,"},{"time":832586,"text":"and I'm looking forward\nto working on them."}]},{"cues":[{"time":835125,"text":"There are reasons for optimism."},{"time":836992,"text":"One reason is,"},{"time":838175,"text":"there is a massive amount of data."},{"time":840067,"text":"Because remember -- I said\nthey're going to read everything"},{"time":842885,"text":"the human race has ever written."},{"time":844455,"text":"Most of what we write about\nis human beings doing things"},{"time":847203,"text":"and other people getting upset about it."},{"time":849141,"text":"So there's a massive amount\nof data to learn from."}]},{"cues":[{"time":851563,"text":"There's also a very\nstrong economic incentive"},{"time":855331,"text":"to get this right."},{"time":856541,"text":"So imagine your domestic robot's at home."},{"time":858566,"text":"You're late from work again\nand the robot has to feed the kids,"},{"time":861657,"text":"and the kids are hungry\nand there's nothing in the fridge."},{"time":864504,"text":"And the robot sees the cat."}]},{"cues":[{"time":867133,"text":"(Laughter)"}]},{"cues":[{"time":868849,"text":"And the robot hasn't quite learned\nthe human value function properly,"},{"time":873063,"text":"so it doesn't understand"},{"time":874338,"text":"the sentimental value of the cat outweighs\nthe nutritional value of the cat."}]},{"cues":[{"time":879206,"text":"(Laughter)"}]},{"cues":[{"time":880325,"text":"So then what happens?"},{"time":882097,"text":"Well, it happens like this:"},{"time":885418,"text":"\"Deranged robot cooks kitty\nfor family dinner.\""},{"time":888406,"text":"That one incident would be the end\nof the domestic robot industry."},{"time":892953,"text":"So there's a huge incentive\nto get this right"},{"time":896349,"text":"long before we reach\nsuperintelligent machines."}]},{"cues":[{"time":900128,"text":"So to summarize:"},{"time":901687,"text":"I'm actually trying to change\nthe definition of AI"},{"time":904592,"text":"so that we have provably\nbeneficial machines."},{"time":907609,"text":"And the principles are:"},{"time":908855,"text":"machines that are altruistic,"},{"time":910277,"text":"that want to achieve only our objectives,"},{"time":913105,"text":"but that are uncertain\nabout what those objectives are,"},{"time":916245,"text":"and will watch all of us"},{"time":918267,"text":"to learn more about what it is\nthat we really want."},{"time":922373,"text":"And hopefully in the process,\nwe will learn to be better people."},{"time":925956,"text":"Thank you very much."}]},{"cues":[{"time":927171,"text":"(Applause)"}]},{"cues":[{"time":930904,"text":"Chris Anderson: So interesting, Stuart."},{"time":932796,"text":"We're going to stand here a bit\nbecause I think they're setting up"},{"time":935990,"text":"for our next speaker."}]},{"cues":[{"time":937165,"text":"A couple of questions."},{"time":938727,"text":"So the idea of programming in ignorance\nseems intuitively really powerful."},{"time":944204,"text":"As you get to superintelligence,"},{"time":945822,"text":"what's going to stop a robot"},{"time":948104,"text":"reading literature and discovering\nthis idea that knowledge"},{"time":950980,"text":"is actually better than ignorance"},{"time":952576,"text":"and still just shifting its own goals\nand rewriting that programming?"}]},{"cues":[{"time":957692,"text":"Stuart Russell: Yes, so we want\nit to learn more, as I said,"},{"time":964072,"text":"about our objectives."},{"time":965383,"text":"It'll only become more certain\nas it becomes more correct,"},{"time":970928,"text":"so the evidence is there"},{"time":972897,"text":"and it's going to be designed\nto interpret it correctly."},{"time":975645,"text":"It will understand, for example,\nthat books are very biased"},{"time":979625,"text":"in the evidence they contain."},{"time":981132,"text":"They only talk about kings and princes"},{"time":983553,"text":"and elite white male people doing stuff."},{"time":986377,"text":"So it's a complicated problem,"},{"time":988497,"text":"but as it learns more about our objectives"},{"time":992393,"text":"it will become more and more useful to us."}]},{"cues":[{"time":994480,"text":"CA: And you couldn't\njust boil it down to one law,"},{"time":997030,"text":"you know, hardwired in:"},{"time":998704,"text":"\"if any human ever tries to switch me off,"},{"time":1002021,"text":"I comply. I comply.\""}]},{"cues":[{"time":1003980,"text":"SR: Absolutely not."},{"time":1005186,"text":"That would be a terrible idea."},{"time":1006709,"text":"So imagine that you have\na self-driving car"},{"time":1009422,"text":"and you want to send your five-year-old"},{"time":1011879,"text":"off to preschool."},{"time":1013077,"text":"Do you want your five-year-old\nto be able to switch off the car"},{"time":1016202,"text":"while it's driving along?"},{"time":1017439,"text":"Probably not."},{"time":1018622,"text":"So it needs to understand how rational\nand sensible the person is."},{"time":1023349,"text":"The more rational the person,"},{"time":1025049,"text":"the more willing you are\nto be switched off."},{"time":1027176,"text":"If the person is completely\nrandom or even malicious,"},{"time":1029743,"text":"then you're less willing\nto be switched off."}]},{"cues":[{"time":1032279,"text":"CA: All right. Stuart, can I just say,"},{"time":1034169,"text":"I really, really hope you\nfigure this out for us."},{"time":1036507,"text":"Thank you so much for that talk.\nThat was amazing."}]},{"cues":[{"time":1038906,"text":"SR: Thank you."}]},{"cues":[{"time":1040097,"text":"(Applause)"}]}]}