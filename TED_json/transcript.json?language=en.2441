{"paragraphs":[{"cues":[{"time":1000,"text":"Today I'm going to talk\nabout technology and society."},{"time":7040,"text":"The Department of Transport\nestimated that last year"},{"time":10760,"text":"35,000 people died\nfrom traffic crashes in the US alone."},{"time":16040,"text":"Worldwide, 1.2 million people\ndie every year in traffic accidents."},{"time":21760,"text":"If there was a way we could eliminate\n90 percent of those accidents,"},{"time":25880,"text":"would you support it?"},{"time":27720,"text":"Of course you would."},{"time":29040,"text":"This is what driverless car technology\npromises to achieve"},{"time":32720,"text":"by eliminating the main\nsource of accidents --"},{"time":35560,"text":"human error."}]},{"cues":[{"time":37920,"text":"Now picture yourself\nin a driverless car in the year 2030,"},{"time":43360,"text":"sitting back and watching\nthis vintage TEDxCambridge video."}]},{"cues":[{"time":46840,"text":"(Laughter)"}]},{"cues":[{"time":49520,"text":"All of a sudden,"},{"time":50760,"text":"the car experiences mechanical failure\nand is unable to stop."},{"time":55360,"text":"If the car continues,"},{"time":57720,"text":"it will crash into a bunch\nof pedestrians crossing the street,"},{"time":63080,"text":"but the car may swerve,"},{"time":65239,"text":"hitting one bystander,"},{"time":67120,"text":"killing them to save the pedestrians."},{"time":70040,"text":"What should the car do,\nand who should decide?"},{"time":73520,"text":"What if instead the car\ncould swerve into a wall,"},{"time":77080,"text":"crashing and killing you, the passenger,"},{"time":80400,"text":"in order to save those pedestrians?"},{"time":83240,"text":"This scenario is inspired\nby the trolley problem,"},{"time":86960,"text":"which was invented\nby philosophers a few decades ago"},{"time":90760,"text":"to think about ethics."}]},{"cues":[{"time":94120,"text":"Now, the way we think\nabout this problem matters."},{"time":96640,"text":"We may for example\nnot think about it at all."},{"time":99280,"text":"We may say this scenario is unrealistic,"},{"time":102680,"text":"incredibly unlikely, or just silly."},{"time":105760,"text":"But I think this criticism\nmisses the point"},{"time":108520,"text":"because it takes\nthe scenario too literally."},{"time":111920,"text":"Of course no accident\nis going to look like this;"},{"time":114680,"text":"no accident has two or three options"},{"time":118040,"text":"where everybody dies somehow."},{"time":121480,"text":"Instead, the car is going\nto calculate something"},{"time":124080,"text":"like the probability of hitting\na certain group of people,"},{"time":129000,"text":"if you swerve one direction\nversus another direction,"},{"time":132360,"text":"you might slightly increase the risk\nto passengers or other drivers"},{"time":135840,"text":"versus pedestrians."},{"time":137400,"text":"It's going to be\na more complex calculation,"},{"time":140480,"text":"but it's still going\nto involve trade-offs,"},{"time":143840,"text":"and trade-offs often require ethics."}]},{"cues":[{"time":147840,"text":"We might say then,\n\"Well, let's not worry about this."},{"time":150600,"text":"Let's wait until technology\nis fully ready and 100 percent safe.\""},{"time":156520,"text":"Suppose that we can indeed\neliminate 90 percent of those accidents,"},{"time":161080,"text":"or even 99 percent in the next 10 years."},{"time":164920,"text":"What if eliminating\nthe last one percent of accidents"},{"time":168120,"text":"requires 50 more years of research?"},{"time":172400,"text":"Should we not adopt the technology?"},{"time":174720,"text":"That's 60 million people\ndead in car accidents"},{"time":179520,"text":"if we maintain the current rate."},{"time":182760,"text":"So the point is,"},{"time":184000,"text":"waiting for full safety is also a choice,"},{"time":187640,"text":"and it also involves trade-offs."}]},{"cues":[{"time":191560,"text":"People online on social media\nhave been coming up with all sorts of ways"},{"time":195920,"text":"to not think about this problem."},{"time":197960,"text":"One person suggested\nthe car should just swerve somehow"},{"time":201200,"text":"in between the passengers --"}]},{"cues":[{"time":203360,"text":"(Laughter)"}]},{"cues":[{"time":204400,"text":"and the bystander."},{"time":205680,"text":"Of course if that's what the car can do,\nthat's what the car should do."},{"time":209920,"text":"We're interested in scenarios\nin which this is not possible."},{"time":213280,"text":"And my personal favorite\nwas a suggestion by a blogger"},{"time":218720,"text":"to have an eject button in the car\nthat you press --"}]},{"cues":[{"time":221760,"text":"(Laughter)"}]},{"cues":[{"time":223000,"text":"just before the car self-destructs."}]},{"cues":[{"time":224691,"text":"(Laughter)"}]},{"cues":[{"time":227840,"text":"So if we acknowledge that cars\nwill have to make trade-offs on the road,"},{"time":234200,"text":"how do we think about those trade-offs,"},{"time":237320,"text":"and how do we decide?"},{"time":238920,"text":"Well, maybe we should run a survey\nto find out what society wants,"},{"time":242080,"text":"because ultimately,"},{"time":243560,"text":"regulations and the law\nare a reflection of societal values."}]},{"cues":[{"time":248040,"text":"So this is what we did."},{"time":249880,"text":"With my collaborators,"},{"time":251520,"text":"Jean-Fran√ßois Bonnefon and Azim Shariff,"},{"time":253880,"text":"we ran a survey"},{"time":255520,"text":"in which we presented people\nwith these types of scenarios."},{"time":258399,"text":"We gave them two options\ninspired by two philosophers:"},{"time":262200,"text":"Jeremy Bentham and Immanuel Kant."},{"time":265600,"text":"Bentham says the car\nshould follow utilitarian ethics:"},{"time":268720,"text":"it should take the action\nthat will minimize total harm --"},{"time":272160,"text":"even if that action will kill a bystander"},{"time":275000,"text":"and even if that action\nwill kill the passenger."},{"time":278120,"text":"Immanuel Kant says the car\nshould follow duty-bound principles,"},{"time":283120,"text":"like \"Thou shalt not kill.\""},{"time":285480,"text":"So you should not take an action\nthat explicitly harms a human being,"},{"time":289960,"text":"and you should let the car take its course"},{"time":292440,"text":"even if that's going to harm more people."}]},{"cues":[{"time":295640,"text":"What do you think?"},{"time":297360,"text":"Bentham or Kant?"},{"time":299760,"text":"Here's what we found."},{"time":301040,"text":"Most people sided with Bentham."},{"time":304160,"text":"So it seems that people\nwant cars to be utilitarian,"},{"time":307960,"text":"minimize total harm,"},{"time":309400,"text":"and that's what we should all do."},{"time":311000,"text":"Problem solved."},{"time":313240,"text":"But there is a little catch."},{"time":315920,"text":"When we asked people\nwhether they would purchase such cars,"},{"time":319680,"text":"they said, \"Absolutely not.\""}]},{"cues":[{"time":321320,"text":"(Laughter)"}]},{"cues":[{"time":323640,"text":"They would like to buy cars\nthat protect them at all costs,"},{"time":327560,"text":"but they want everybody else\nto buy cars that minimize harm."}]},{"cues":[{"time":331200,"text":"(Laughter)"}]},{"cues":[{"time":334720,"text":"We've seen this problem before."},{"time":336600,"text":"It's called a social dilemma."},{"time":339160,"text":"And to understand the social dilemma,"},{"time":341000,"text":"we have to go a little bit\nback in history."},{"time":344000,"text":"In the 1800s,"},{"time":346600,"text":"English economist William Forster Lloyd\npublished a pamphlet"},{"time":350360,"text":"which describes the following scenario."},{"time":352600,"text":"You have a group of farmers --"},{"time":354280,"text":"English farmers --"},{"time":355640,"text":"who are sharing a common land\nfor their sheep to graze."},{"time":359520,"text":"Now, if each farmer\nbrings a certain number of sheep --"},{"time":362120,"text":"let's say three sheep --"},{"time":363640,"text":"the land will be rejuvenated,"},{"time":365760,"text":"the farmers are happy,"},{"time":367000,"text":"the sheep are happy,"},{"time":368640,"text":"everything is good."},{"time":370440,"text":"Now, if one farmer brings one extra sheep,"},{"time":373800,"text":"that farmer will do slightly better,\nand no one else will be harmed."},{"time":379160,"text":"But if every farmer made\nthat individually rational decision,"},{"time":383840,"text":"the land will be overrun,\nand it will be depleted"},{"time":387360,"text":"to the detriment of all the farmers,"},{"time":389560,"text":"and of course,\nto the detriment of the sheep."}]},{"cues":[{"time":392720,"text":"We see this problem in many places:"},{"time":397080,"text":"in the difficulty of managing overfishing,"},{"time":400280,"text":"or in reducing carbon emissions\nto mitigate climate change."},{"time":407160,"text":"When it comes to the regulation\nof driverless cars,"},{"time":411080,"text":"the common land now\nis basically public safety --"},{"time":415440,"text":"that's the common good --"},{"time":417400,"text":"and the farmers are the passengers"},{"time":419400,"text":"or the car owners who are choosing\nto ride in those cars."},{"time":424960,"text":"And by making the individually\nrational choice"},{"time":427600,"text":"of prioritizing their own safety,"},{"time":430440,"text":"they may collectively be\ndiminishing the common good,"},{"time":433600,"text":"which is minimizing total harm."},{"time":438320,"text":"It's called the tragedy of the commons,"},{"time":440480,"text":"traditionally,"},{"time":441800,"text":"but I think in the case\nof driverless cars,"},{"time":444920,"text":"the problem may be\na little bit more insidious"},{"time":447800,"text":"because there is not necessarily\nan individual human being"},{"time":451320,"text":"making those decisions."},{"time":453040,"text":"So car manufacturers\nmay simply program cars"},{"time":456360,"text":"that will maximize safety\nfor their clients,"},{"time":460080,"text":"and those cars may learn\nautomatically on their own"},{"time":463080,"text":"that doing so requires slightly\nincreasing risk for pedestrians."},{"time":467520,"text":"So to use the sheep metaphor,"},{"time":468960,"text":"it's like we now have electric sheep\nthat have a mind of their own."}]},{"cues":[{"time":472600,"text":"(Laughter)"}]},{"cues":[{"time":474080,"text":"And they may go and graze\neven if the farmer doesn't know it."}]},{"cues":[{"time":478640,"text":"So this is what we may call\nthe tragedy of the algorithmic commons,"},{"time":482640,"text":"and if offers new types of challenges."},{"time":490520,"text":"Typically, traditionally,"},{"time":492440,"text":"we solve these types\nof social dilemmas using regulation,"},{"time":495800,"text":"so either governments\nor communities get together,"},{"time":498560,"text":"and they decide collectively\nwhat kind of outcome they want"},{"time":502320,"text":"and what sort of constraints\non individual behavior"},{"time":505000,"text":"they need to implement."},{"time":507600,"text":"And then using monitoring and enforcement,"},{"time":510240,"text":"they can make sure\nthat the public good is preserved."},{"time":513440,"text":"So why don't we just,"},{"time":515039,"text":"as regulators,"},{"time":516559,"text":"require that all cars minimize harm?"},{"time":519480,"text":"After all, this is\nwhat people say they want."},{"time":523200,"text":"And more importantly,"},{"time":524640,"text":"I can be sure that as an individual,"},{"time":527760,"text":"if I buy a car that may\nsacrifice me in a very rare case,"},{"time":531640,"text":"I'm not the only sucker doing that"},{"time":533320,"text":"while everybody else\nenjoys unconditional protection."}]},{"cues":[{"time":537120,"text":"In our survey, we did ask people\nwhether they would support regulation"},{"time":540480,"text":"and here's what we found."},{"time":542360,"text":"First of all, people\nsaid no to regulation;"},{"time":547280,"text":"and second, they said,"},{"time":548560,"text":"\"Well if you regulate cars to do this\nand to minimize total harm,"},{"time":552520,"text":"I will not buy those cars.\""},{"time":555400,"text":"So ironically,"},{"time":556800,"text":"by regulating cars to minimize harm,"},{"time":560320,"text":"we may actually end up with more harm"},{"time":563040,"text":"because people may not\nopt into the safer technology"},{"time":566720,"text":"even if it's much safer\nthan human drivers."}]},{"cues":[{"time":570360,"text":"I don't have the final\nanswer to this riddle,"},{"time":573800,"text":"but I think as a starting point,"},{"time":575400,"text":"we need society to come together"},{"time":578720,"text":"to decide what trade-offs\nwe are comfortable with"},{"time":582360,"text":"and to come up with ways\nin which we can enforce those trade-offs."}]},{"cues":[{"time":586520,"text":"As a starting point,\nmy brilliant students,"},{"time":589080,"text":"Edmond Awad and Sohan Dsouza,"},{"time":591560,"text":"built the Moral Machine website,"},{"time":594200,"text":"which generates random scenarios at you --"},{"time":598080,"text":"basically a bunch\nof random dilemmas in a sequence"},{"time":600560,"text":"where you have to choose what\nthe car should do in a given scenario."},{"time":605040,"text":"And we vary the ages and even\nthe species of the different victims."},{"time":611040,"text":"So far we've collected\nover five million decisions"},{"time":614760,"text":"by over one million people worldwide"},{"time":618400,"text":"from the website."},{"time":620360,"text":"And this is helping us\nform an early picture"},{"time":622800,"text":"of what trade-offs\npeople are comfortable with"},{"time":625440,"text":"and what matters to them --"},{"time":627360,"text":"even across cultures."},{"time":630240,"text":"But more importantly,"},{"time":631760,"text":"doing this exercise\nis helping people recognize"},{"time":635160,"text":"the difficulty of making those choices"},{"time":638000,"text":"and that the regulators\nare tasked with impossible choices."},{"time":643360,"text":"And maybe this will help us as a society\nunderstand the kinds of trade-offs"},{"time":646960,"text":"that will be implemented\nultimately in regulation."}]},{"cues":[{"time":650040,"text":"And indeed, I was very happy to hear"},{"time":651800,"text":"that the first set of regulations"},{"time":653840,"text":"that came from\nthe Department of Transport --"},{"time":656000,"text":"announced last week --"},{"time":657400,"text":"included a 15-point checklist\nfor all carmakers to provide,"},{"time":664000,"text":"and number 14 was ethical consideration --"},{"time":667280,"text":"how are you going to deal with that."},{"time":671800,"text":"We also have people\nreflect on their own decisions"},{"time":674480,"text":"by giving them summaries\nof what they chose."},{"time":678440,"text":"I'll give you one example --"},{"time":680120,"text":"I'm just going to warn you\nthat this is not your typical example,"},{"time":683680,"text":"your typical user."},{"time":685080,"text":"This is the most sacrificed and the most\nsaved character for this person."}]},{"cues":[{"time":688720,"text":"(Laughter)"}]},{"cues":[{"time":694680,"text":"Some of you may agree with him,"},{"time":696600,"text":"or her, we don't know."},{"time":700480,"text":"But this person also seems to slightly\nprefer passengers over pedestrians"},{"time":706640,"text":"in their choices"},{"time":708760,"text":"and is very happy to punish jaywalking."}]},{"cues":[{"time":711600,"text":"(Laughter)"}]},{"cues":[{"time":717320,"text":"So let's wrap up."},{"time":718559,"text":"We started with the question --\nlet's call it the ethical dilemma --"},{"time":722000,"text":"of what the car should do\nin a specific scenario:"},{"time":725080,"text":"swerve or stay?"},{"time":727240,"text":"But then we realized\nthat the problem was a different one."},{"time":730000,"text":"It was the problem of how to get\nsociety to agree on and enforce"},{"time":734560,"text":"the trade-offs they're comfortable with."},{"time":736520,"text":"It's a social dilemma."}]},{"cues":[{"time":737800,"text":"In the 1940s, Isaac Asimov\nwrote his famous laws of robotics --"},{"time":742840,"text":"the three laws of robotics."},{"time":745240,"text":"A robot may not harm a human being,"},{"time":747720,"text":"a robot may not disobey a human being,"},{"time":750280,"text":"and a robot may not allow\nitself to come to harm --"},{"time":753560,"text":"in this order of importance."},{"time":756360,"text":"But after 40 years or so"},{"time":758520,"text":"and after so many stories\npushing these laws to the limit,"},{"time":762280,"text":"Asimov introduced the zeroth law"},{"time":766000,"text":"which takes precedence above all,"},{"time":768280,"text":"and it's that a robot\nmay not harm humanity as a whole."},{"time":772480,"text":"I don't know what this means\nin the context of driverless cars"},{"time":776880,"text":"or any specific situation,"},{"time":779640,"text":"and I don't know how we can implement it,"},{"time":781880,"text":"but I think that by recognizing"},{"time":783440,"text":"that the regulation of driverless cars\nis not only a technological problem"},{"time":789600,"text":"but also a societal cooperation problem,"},{"time":793800,"text":"I hope that we can at least begin\nto ask the right questions."}]},{"cues":[{"time":797200,"text":"Thank you."}]},{"cues":[{"time":798440,"text":"(Applause)"}]}]}