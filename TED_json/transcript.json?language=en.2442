{"paragraphs":[{"cues":[{"time":975,"text":"Algorithms are everywhere."},{"time":4111,"text":"They sort and separate\nthe winners from the losers."},{"time":8019,"text":"The winners get the job"},{"time":10307,"text":"or a good credit card offer."},{"time":12074,"text":"The losers don't even get an interview"},{"time":15590,"text":"or they pay more for insurance."},{"time":18197,"text":"We're being scored with secret formulas\nthat we don't understand"},{"time":22675,"text":"that often don't have systems of appeal."},{"time":27240,"text":"That begs the question:"},{"time":28560,"text":"What if the algorithms are wrong?"}]},{"cues":[{"time":33100,"text":"To build an algorithm you need two things:"},{"time":35164,"text":"you need data, what happened in the past,"},{"time":37169,"text":"and a definition of success,"},{"time":38754,"text":"the thing you're looking for\nand often hoping for."},{"time":41235,"text":"You train an algorithm\nby looking, figuring out."},{"time":46296,"text":"The algorithm figures out\nwhat is associated with success."},{"time":49739,"text":"What situation leads to success?"}]},{"cues":[{"time":52881,"text":"Actually, everyone uses algorithms."},{"time":54667,"text":"They just don't formalize them\nin written code."},{"time":57409,"text":"Let me give you an example."},{"time":58781,"text":"I use an algorithm every day\nto make a meal for my family."},{"time":62121,"text":"The data I use"},{"time":64394,"text":"is the ingredients in my kitchen,"},{"time":66077,"text":"the time I have,"},{"time":67628,"text":"the ambition I have,"},{"time":68885,"text":"and I curate that data."},{"time":70618,"text":"I don't count those little packages\nof ramen noodles as food."}]},{"cues":[{"time":74893,"text":"(Laughter)"}]},{"cues":[{"time":76786,"text":"My definition of success is:"},{"time":78655,"text":"a meal is successful\nif my kids eat vegetables."},{"time":82181,"text":"It's very different\nfrom if my youngest son were in charge."},{"time":85059,"text":"He'd say success is if\nhe gets to eat lots of Nutella."},{"time":89179,"text":"But I get to choose success."},{"time":91429,"text":"I am in charge. My opinion matters."},{"time":94160,"text":"That's the first rule of algorithms."}]},{"cues":[{"time":96859,"text":"Algorithms are opinions embedded in code."},{"time":101562,"text":"It's really different from what you think\nmost people think of algorithms."},{"time":105249,"text":"They think algorithms are objective\nand true and scientific."},{"time":110387,"text":"That's a marketing trick."},{"time":113269,"text":"It's also a marketing trick"},{"time":115418,"text":"to intimidate you with algorithms,"},{"time":118596,"text":"to make you trust and fear algorithms"},{"time":122281,"text":"because you trust and fear mathematics."},{"time":125567,"text":"A lot can go wrong when we put\nblind faith in big data."}]},{"cues":[{"time":131684,"text":"This is Kiri Soares.\nShe's a high school principal in Brooklyn."},{"time":135081,"text":"In 2011, she told me\nher teachers were being scored"},{"time":137691,"text":"with a complex, secret algorithm"},{"time":140442,"text":"called the \"value-added model.\""},{"time":142505,"text":"I told her, \"Well, figure out\nwhat the formula is, show it to me."},{"time":145621,"text":"I'm going to explain it to you.\""},{"time":147186,"text":"She said, \"Well, I tried\nto get the formula,"},{"time":149351,"text":"but my Department of Education contact\ntold me it was math"},{"time":152147,"text":"and I wouldn't understand it.\""}]},{"cues":[{"time":155266,"text":"It gets worse."},{"time":156628,"text":"The New York Post filed\na Freedom of Information Act request,"},{"time":160182,"text":"got all the teachers' names\nand all their scores"},{"time":163165,"text":"and they published them\nas an act of teacher-shaming."},{"time":167084,"text":"When I tried to get the formulas,\nthe source code, through the same means,"},{"time":170968,"text":"I was told I couldn't."},{"time":173141,"text":"I was denied."},{"time":174401,"text":"I later found out"},{"time":175599,"text":"that nobody in New York City\nhad access to that formula."},{"time":178489,"text":"No one understood it."},{"time":181929,"text":"Then someone really smart\ngot involved, Gary Rubinstein."},{"time":185177,"text":"He found 665 teachers\nfrom that New York Post data"},{"time":188822,"text":"that actually had two scores."},{"time":190712,"text":"That could happen if they were teaching"},{"time":192617,"text":"seventh grade math and eighth grade math."},{"time":195080,"text":"He decided to plot them."},{"time":196642,"text":"Each dot represents a teacher."}]},{"cues":[{"time":199104,"text":"(Laughter)"}]},{"cues":[{"time":201507,"text":"What is that?"}]},{"cues":[{"time":203052,"text":"(Laughter)"}]},{"cues":[{"time":204353,"text":"That should never have been used\nfor individual assessment."},{"time":207823,"text":"It's almost a random number generator."}]},{"cues":[{"time":209773,"text":"(Applause)"}]},{"cues":[{"time":212743,"text":"But it was."},{"time":213929,"text":"This is Sarah Wysocki."},{"time":215129,"text":"She got fired, along\nwith 205 other teachers,"},{"time":217328,"text":"from the Washington, DC school district,"},{"time":220014,"text":"even though she had great\nrecommendations from her principal"},{"time":222947,"text":"and the parents of her kids."}]},{"cues":[{"time":225390,"text":"I know what a lot\nof you guys are thinking,"},{"time":227446,"text":"especially the data scientists,\nthe AI experts here."},{"time":229957,"text":"You're thinking, \"Well, I would never make\nan algorithm that inconsistent.\""},{"time":234853,"text":"But algorithms can go wrong,"},{"time":236560,"text":"even have deeply destructive effects\nwith good intentions."},{"time":242531,"text":"And whereas an airplane\nthat's designed badly"},{"time":244934,"text":"crashes to the earth and everyone sees it,"},{"time":246959,"text":"an algorithm designed badly"},{"time":250245,"text":"can go on for a long time,\nsilently wreaking havoc."}]},{"cues":[{"time":255748,"text":"This is Roger Ailes."}]},{"cues":[{"time":257342,"text":"(Laughter)"}]},{"cues":[{"time":260524,"text":"He founded Fox News in 1996."},{"time":263436,"text":"More than 20 women complained\nabout sexual harassment."},{"time":266041,"text":"They said they weren't allowed\nto succeed at Fox News."},{"time":269300,"text":"He was ousted last year,\nbut we've seen recently"},{"time":271844,"text":"that the problems have persisted."},{"time":275654,"text":"That begs the question:"},{"time":277078,"text":"What should Fox News do\nto turn over another leaf?"}]},{"cues":[{"time":281245,"text":"Well, what if they replaced\ntheir hiring process"},{"time":284310,"text":"with a machine-learning algorithm?"},{"time":285988,"text":"That sounds good, right?"},{"time":287607,"text":"Think about it."},{"time":288931,"text":"The data, what would the data be?"},{"time":291060,"text":"A reasonable choice would be the last\n21 years of applications to Fox News."},{"time":296031,"text":"Reasonable."},{"time":297557,"text":"What about the definition of success?"},{"time":299921,"text":"Reasonable choice would be,"},{"time":301269,"text":"well, who is successful at Fox News?"},{"time":303071,"text":"I guess someone who, say,\nstayed there for four years"},{"time":306675,"text":"and was promoted at least once."},{"time":308816,"text":"Sounds reasonable."},{"time":310401,"text":"And then the algorithm would be trained."},{"time":312779,"text":"It would be trained to look for people\nto learn what led to success,"},{"time":317219,"text":"what kind of applications\nhistorically led to success"},{"time":321561,"text":"by that definition."},{"time":324200,"text":"Now think about what would happen"},{"time":325999,"text":"if we applied that\nto a current pool of applicants."},{"time":329119,"text":"It would filter out women"},{"time":331663,"text":"because they do not look like people\nwho were successful in the past."}]},{"cues":[{"time":339752,"text":"Algorithms don't make things fair"},{"time":342313,"text":"if you just blithely,\nblindly apply algorithms."},{"time":345031,"text":"They don't make things fair."},{"time":346537,"text":"They repeat our past practices,"},{"time":348689,"text":"our patterns."},{"time":349896,"text":"They automate the status quo."},{"time":352718,"text":"That would be great\nif we had a perfect world,"},{"time":355905,"text":"but we don't."},{"time":357241,"text":"And I'll add that most companies\ndon't have embarrassing lawsuits,"},{"time":362446,"text":"but the data scientists in those companies"},{"time":365058,"text":"are told to follow the data,"},{"time":367271,"text":"to focus on accuracy."},{"time":370273,"text":"Think about what that means."},{"time":371678,"text":"Because we all have bias,\nit means they could be codifying sexism"},{"time":375729,"text":"or any other kind of bigotry."}]},{"cues":[{"time":379488,"text":"Thought experiment,"},{"time":380933,"text":"because I like them:"},{"time":383574,"text":"an entirely segregated society --"},{"time":388247,"text":"racially segregated, all towns,\nall neighborhoods"},{"time":391599,"text":"and where we send the police\nonly to the minority neighborhoods"},{"time":394660,"text":"to look for crime."},{"time":396451,"text":"The arrest data would be very biased."},{"time":399851,"text":"What if, on top of that,\nwe found the data scientists"},{"time":402450,"text":"and paid the data scientists to predict\nwhere the next crime would occur?"},{"time":407275,"text":"Minority neighborhood."},{"time":409285,"text":"Or to predict who the next\ncriminal would be?"},{"time":412888,"text":"A minority."},{"time":415949,"text":"The data scientists would brag\nabout how great and how accurate"},{"time":419514,"text":"their model would be,"},{"time":420835,"text":"and they'd be right."}]},{"cues":[{"time":423951,"text":"Now, reality isn't that drastic,\nbut we do have severe segregations"},{"time":428590,"text":"in many cities and towns,"},{"time":429901,"text":"and we have plenty of evidence"},{"time":431818,"text":"of biased policing\nand justice system data."},{"time":435632,"text":"And we actually do predict hotspots,"},{"time":438471,"text":"places where crimes will occur."},{"time":440401,"text":"And we do predict, in fact,\nthe individual criminality,"},{"time":444291,"text":"the criminality of individuals."},{"time":446972,"text":"The news organization ProPublica\nrecently looked into"},{"time":450959,"text":"one of those \"recidivism risk\" algorithms,"},{"time":453007,"text":"as they're called,"},{"time":454194,"text":"being used in Florida\nduring sentencing by judges."},{"time":458411,"text":"Bernard, on the left, the black man,\nwas scored a 10 out of 10."},{"time":463179,"text":"Dylan, on the right, 3 out of 10."},{"time":465210,"text":"10 out of 10, high risk.\n3 out of 10, low risk."},{"time":468598,"text":"They were both brought in\nfor drug possession."},{"time":471007,"text":"They both had records,"},{"time":472185,"text":"but Dylan had a felony"},{"time":475015,"text":"but Bernard didn't."},{"time":477818,"text":"This matters, because\nthe higher score you are,"},{"time":480908,"text":"the more likely you're being given\na longer sentence."}]},{"cues":[{"time":486294,"text":"What's going on?"},{"time":488526,"text":"Data laundering."},{"time":490930,"text":"It's a process by which\ntechnologists hide ugly truths"},{"time":495381,"text":"inside black box algorithms"},{"time":497226,"text":"and call them objective;"},{"time":499320,"text":"call them meritocratic."},{"time":503118,"text":"When they're secret,\nimportant and destructive,"},{"time":505527,"text":"I've coined a term for these algorithms:"},{"time":508038,"text":"\"weapons of math destruction.\""}]},{"cues":[{"time":510061,"text":"(Laughter)"}]},{"cues":[{"time":511649,"text":"(Applause)"}]},{"cues":[{"time":514727,"text":"They're everywhere,\nand it's not a mistake."},{"time":517695,"text":"These are private companies\nbuilding private algorithms"},{"time":521442,"text":"for private ends."},{"time":523214,"text":"Even the ones I talked about\nfor teachers and the public police,"},{"time":526452,"text":"those were built by private companies"},{"time":528345,"text":"and sold to the government institutions."},{"time":530600,"text":"They call it their \"secret sauce\" --"},{"time":532497,"text":"that's why they can't tell us about it."},{"time":534649,"text":"It's also private power."},{"time":537924,"text":"They are profiting for wielding\nthe authority of the inscrutable."},{"time":545114,"text":"Now you might think,\nsince all this stuff is private"},{"time":548072,"text":"and there's competition,"},{"time":549254,"text":"maybe the free market\nwill solve this problem."},{"time":551584,"text":"It won't."},{"time":552857,"text":"There's a lot of money\nto be made in unfairness."}]},{"cues":[{"time":557127,"text":"Also, we're not economic rational agents."},{"time":561031,"text":"We all are biased."},{"time":562960,"text":"We're all racist and bigoted\nin ways that we wish we weren't,"},{"time":566361,"text":"in ways that we don't even know."},{"time":569352,"text":"We know this, though, in aggregate,"},{"time":572457,"text":"because sociologists\nhave consistently demonstrated this"},{"time":575701,"text":"with these experiments they build,"},{"time":577390,"text":"where they send a bunch\nof applications to jobs out,"},{"time":579982,"text":"equally qualified but some\nhave white-sounding names"},{"time":582507,"text":"and some have black-sounding names,"},{"time":584237,"text":"and it's always disappointing,\nthe results -- always."}]},{"cues":[{"time":587510,"text":"So we are the ones that are biased,"},{"time":589305,"text":"and we are injecting those biases\ninto the algorithms"},{"time":592758,"text":"by choosing what data to collect,"},{"time":594594,"text":"like I chose not to think\nabout ramen noodles --"},{"time":597361,"text":"I decided it was irrelevant."},{"time":599010,"text":"But by trusting the data that's actually\npicking up on past practices"},{"time":604718,"text":"and by choosing the definition of success,"},{"time":606756,"text":"how can we expect the algorithms\nto emerge unscathed?"},{"time":610763,"text":"We can't. We have to check them."},{"time":614165,"text":"We have to check them for fairness."}]},{"cues":[{"time":615898,"text":"The good news is,\nwe can check them for fairness."},{"time":618633,"text":"Algorithms can be interrogated,"},{"time":622009,"text":"and they will tell us\nthe truth every time."},{"time":624067,"text":"And we can fix them.\nWe can make them better."},{"time":626584,"text":"I call this an algorithmic audit,"},{"time":628983,"text":"and I'll walk you through it."}]},{"cues":[{"time":630686,"text":"First, data integrity check."},{"time":634132,"text":"For the recidivism risk\nalgorithm I talked about,"},{"time":637582,"text":"a data integrity check would mean\nwe'd have to come to terms with the fact"},{"time":641179,"text":"that in the US, whites and blacks\nsmoke pot at the same rate"},{"time":644729,"text":"but blacks are far more likely\nto be arrested --"},{"time":647238,"text":"four or five times more likely,\ndepending on the area."},{"time":651317,"text":"What is that bias looking like\nin other crime categories,"},{"time":654167,"text":"and how do we account for it?"}]},{"cues":[{"time":656162,"text":"Second, we should think about\nthe definition of success,"},{"time":659225,"text":"audit that."},{"time":660630,"text":"Remember -- with the hiring\nalgorithm? We talked about it."},{"time":663406,"text":"Someone who stays for four years\nand is promoted once?"},{"time":666595,"text":"Well, that is a successful employee,"},{"time":668388,"text":"but it's also an employee\nthat is supported by their culture."},{"time":672089,"text":"That said, also it can be quite biased."},{"time":674039,"text":"We need to separate those two things."},{"time":676128,"text":"We should look to\nthe blind orchestra audition"},{"time":678578,"text":"as an example."},{"time":679798,"text":"That's where the people auditioning\nare behind a sheet."},{"time":682946,"text":"What I want to think about there"},{"time":684901,"text":"is the people who are listening\nhave decided what's important"},{"time":688342,"text":"and they've decided what's not important,"},{"time":690395,"text":"and they're not getting\ndistracted by that."},{"time":692961,"text":"When the blind orchestra\nauditions started,"},{"time":695734,"text":"the number of women in orchestras\nwent up by a factor of five."}]},{"cues":[{"time":700253,"text":"Next, we have to consider accuracy."},{"time":703233,"text":"This is where the value-added model\nfor teachers would fail immediately."},{"time":707578,"text":"No algorithm is perfect, of course,"},{"time":710620,"text":"so we have to consider\nthe errors of every algorithm."},{"time":714836,"text":"How often are there errors,\nand for whom does this model fail?"},{"time":719850,"text":"What is the cost of that failure?"}]},{"cues":[{"time":722434,"text":"And finally, we have to consider"},{"time":725973,"text":"the long-term effects of algorithms,"},{"time":728866,"text":"the feedback loops that are engendering."},{"time":731586,"text":"That sounds abstract,"},{"time":732846,"text":"but imagine if Facebook engineers\nhad considered that"},{"time":736270,"text":"before they decided to show us\nonly things that our friends had posted."}]},{"cues":[{"time":741761,"text":"I have two more messages,\none for the data scientists out there."},{"time":745450,"text":"Data scientists: we should\nnot be the arbiters of truth."},{"time":749520,"text":"We should be translators\nof ethical discussions that happen"},{"time":753327,"text":"in larger society."}]},{"cues":[{"time":755579,"text":"(Applause)"}]},{"cues":[{"time":757736,"text":"And the rest of you,"},{"time":760011,"text":"the non-data scientists:"},{"time":761431,"text":"this is not a math test."},{"time":763632,"text":"This is a political fight."},{"time":766587,"text":"We need to demand accountability\nfor our algorithmic overlords."}]},{"cues":[{"time":772118,"text":"(Applause)"}]},{"cues":[{"time":773641,"text":"The era of blind faith\nin big data must end."}]},{"cues":[{"time":777890,"text":"Thank you very much."}]},{"cues":[{"time":779081,"text":"(Applause)"}]}]}